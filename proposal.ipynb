{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89601249",
   "metadata": {},
   "source": [
    "# CS344 Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9e7b95",
   "metadata": {},
   "source": [
    "Team Members: Daniel Kwik, Peter Peng\n",
    "- We will schedule regular times to meet up and work on the project together since it is difficult to do this kind of task in a distributed way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d0f02c",
   "metadata": {},
   "source": [
    "## Vision\n",
    "This project is to replicate a [Kaggle competition](https://www.kaggle.com/competitions/challenges-in-representation-learning-facial-expression-recognition-challenge/overview) to build a Facial Expression Recognition (FER) model to classify images of faces with emotions. Following the guidelines of the competition, we will use the data provided to categorize each face based on 7 different types of emotions: Anger, Disgust, Fear, Happiness, Sadness, Surprise, and Neutral. We find it interesting because knowing people's emotions with an AI can open the door for a lot of practical opportunities, for example, an emotion tracker application. The goal of this project is to train a state-of-the-art FER model, and a successful project outcome will be attaining about 73% of accuracy of the testing set, as shown in this [paper](https://arxiv.org/ftp/arxiv/papers/2105/2105.03588.pdf), which is surpassing the single-network state-of-the-art performance during the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab51806",
   "metadata": {},
   "source": [
    "## Background\n",
    "We are using the [FER2013](https://www.kaggle.com/competitions/challenges-in-representation-learning-facial-expression-recognition-challenge/data) dataset. This dataset is consisted of 48x48 pixel grayscale images of faces which have already been edited so that they are centered and occupies the same amount of space. There are 28,709 images in the training set, and 3589 images in the test set. We chose to do a Kaggle competition in part because we will have access to many different competition entries so that we can learn how different implementations lead to different results. If this dataset does not work, we can use the [Google Facial Expression dataset](https://research.google/tools/datasets/google-facial-expression/) instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4479c5f1",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "We can build on the code found in the Kaggle competition and use it as a map to learn how to build something similar in fast.ai. We will adopt the VGGNet architecture using Convolutional Neural Networks. Namely, starting with data augmentation with scaling/shifting the images by 20% and rotating it by 10%, all this done randomly. We will use a learning rate of 0.9 and a weight decay of 0.0001. We will follow the state-of-the-art variant of VGGNet for this task which is 4 convolutional stages and 3 fully connected layers, with each stage containing 2 convolutional blocks and a max-pooling. There are 3 fully connected layers. We will also explore various multi-label image classifying techniques introduced in Ch. 6 of Fast AI textbook to try improving the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd042f",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff70a2c",
   "metadata": {},
   "source": [
    "## Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7cfa4c",
   "metadata": {},
   "source": [
    "## Steps Taken\n",
    "- We reviewed Ch. 6 of the Fast AI textbook on multi-label image classifier.\n",
    "- We looked through the methods used in this [paper](https://arxiv.org/ftp/arxiv/papers/2105/2105.03588.pdf)\n",
    "- We read about how other people have approached this competition on Kaggle's site."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf3ad06",
   "metadata": {},
   "source": [
    "## Help Needed\n",
    "- We will probably need help to implement some of the more advanced techniques used in the paper and some other sources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
