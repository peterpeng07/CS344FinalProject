{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d107df80",
   "metadata": {},
   "source": [
    "# CS344 Final Project - Peter Peng & Daniel Kwik\n",
    "# Training a facial emotion classifier with fast.ai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f7f1f2",
   "metadata": {},
   "source": [
    "## Goal\n",
    "This project is to replicate a [Kaggle competition](https://www.kaggle.com/competitions/challenges-in-representation-learning-facial-expression-recognition-challenge/overview) to build a Facial Expression Recognition (FER) model to classify images of faces with emotions using Fast.AI's library. Following the guidelines of the competition, we will use the data provided to categorize each face based on 7 different types of emotions: Anger, Disgust, Fear, Happiness, Sadness, Surprise, and Neutral. We find it interesting because knowing people's emotions with an AI can open the door for a lot of practical opportunities, for example, an emotion tracker application. The goal of this project is to train a state-of-the-art FER model, and a successful project outcome will be attaining about 73.28% of accuracy of the testing set, as shown in this [paper](https://arxiv.org/ftp/arxiv/papers/2105/2105.03588.pdf), which is surpassing the single-network state-of-the-art performance during the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6fe2fb",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We are using the [FER2013](https://www.kaggle.com/competitions/challenges-in-representation-learning-facial-expression-recognition-challenge/data) dataset. We chose to do a Kaggle competition in part because we will have access to many different competition entries so that we can learn how different implementations lead to different results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8272d4b",
   "metadata": {},
   "source": [
    "## Approach\n",
    "We will attempt to replicate the results from the state-of-the-art FER model using fast.ai's library. We first tried out using different architectures, namely RESNET34, VGG16_BN, VGG16_BN with an SGD optimizer function, as well as a VGG19_BN. We chose the model with the best results and experimented with data augmentation, specifically resizing the images from 48x48 to 160x160. Finally, we experimented the use of [DeepAI's Colorized model](https://deepai.org/machine-learning-model/colorizer) to colorize our images before training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc03f7",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "The dataset is consisted of 48x48 pixel grayscale images of faces which have already been edited so that they are centered and occupies the same amount of space. There are 28,709 images in the training set, and 3589 images in the test set. Upon picking samples at random, we learned that a variety of ethnic, genders, and age groups are represented in the dataset. Some images have people wearing accessories (e.g sunglasses), others have faces that are at an angle (not facing dead-on), and others still are illustration or drawings of faces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ea8cb",
   "metadata": {},
   "source": [
    "## Modeling Setup\n",
    "After some experiment, we found that using the VGG16_bn architecture with Cross Entropy Loss as our loss function, and using accuracy as our metric. We attempt to use the pre-trained VGG16 model to target our 7 emotions, namely Anger, Disgust, Fear, Happiness, Sadness, Surprise, and Neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a06249a",
   "metadata": {},
   "source": [
    "## Validation Approach\n",
    "In this project we use the train/test split validation approach. Since we used a competition dataset, it comes with our training and testing data already split for us, with 28709 images for training, and 7178 for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a6e1f",
   "metadata": {},
   "source": [
    "## Baseline Results\n",
    "As a baseline, we implemented a RESNET34 architecture using Cross Entropy Loss as our loss function. We used fast.ai's learning rate finder to decide on a learning rate of 0.004, and fine tuned our model with 8 epochs, giving us an accuracy of 0.625801. This was a little far away from our goal of achieving 73.28% accuracy, but it was a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c125799",
   "metadata": {},
   "source": [
    "## Improving Results\n",
    "#### Experiments with different architectures & optimizer functions\n",
    "We experimented with the following combinations of architectures, optimizer functions, epochs, and learning rates and achieved the following results:\n",
    "\n",
    "| Architecture | Optimizer Function | Epochs | Learning Rate | Accuracy |\n",
    "|--------------|--------------------|--------|---------------|----------|\n",
    "| RESNET34     | Adam               | 8      | 0.004         | 0.625801 |\n",
    "| RESNET34     | Adam               | 50     | 0.003         | 0.612705 |\n",
    "| VGG16        | Adam               | 8      | 0.004         | 0.662023 |\n",
    "| VGG16        | Adam               | 100    | 0.004         | 0.670103 |\n",
    "| VGG16        | SGD                | 8      | 0.015         | 0.496656 |\n",
    "| VGG19        | Adam               | 8      | 0.0025        | 0.660908 |\n",
    "| VGG19        | Adam               | 20     | 0.0025        | 0.656590 |\n",
    "| VGG19        | Adam               | 20     | 2.5e-6        | 0.652828 |\n",
    "| VGG19        | Adam               | 20     | 0.0015        | 0.662719 |\n",
    "| **VGG16**        | **Adam**               | **40**     | **0.0012/3e-04**        | **0.701449/0.702424** |\n",
    "\n",
    "From our experiments, we determined that VGG16 with Cross Entropy Loss as our loss function was the best setup to train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba3b21c",
   "metadata": {},
   "source": [
    "#### Data Augmentation\n",
    "Next, we used our model architecture and performed some data augmentation on our images. We did this by using fast.ai's [default data augmentation algorithm](https://rdrr.io/cran/fastai/man/aug_transforms.html)  and manually resized the images from 48x48 to 160x160. We retrained the model and got our best accuracy result of **0.702424**.\n",
    "\n",
    "\n",
    "#### Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6920f990",
   "metadata": {},
   "source": [
    "#### Colorization using DeepAI\n",
    "Additionally, we experimented with adding color to our originally grayscale images to see if our model performed better. We did this using [Deep AI's Colorize Model](https://deepai.org/machine-learning-model/colorizer). Training the model only resulted in an accuracy of 0.640568. When we inspected the colorized images, we found that there was not much different in color, nor were they true to real world color of human faces, which is probably why it did not train well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f4470",
   "metadata": {},
   "source": [
    "## Additional Testing\n",
    "Just to see how our model performs on our own faces, we decided to take a picture of our own faces with various expressions to see how the model would perform. Overall, the model performed quite well with some errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689009a5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Overall, we found that our best model (70.24% accuracy) that came pretty close to the state-of-the-art model (73.28% accuracy) was the VGG16 architecture, using Cross Entropy Loss, trained on 20 epochs with a learning rate of 0.002. Unsurprisingly, this was the same architecture used for the state-of-the-art model, although they used a more advanced learning rate scheduler.\n",
    "\n",
    "We believe that while we did not reach state-of-the-art, we achieved our goal of transfer learning and successfully implemented a close model through fast.ai's library. In the future, we would like to continue experimenting with more sophisticated optimizer algorithms using learning rate schedulers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
